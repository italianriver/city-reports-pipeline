{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db6d318-7adc-4ed6-988b-e1bb7864b175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78af32f4-af2f-454e-bfa0-60721f0f5547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vars = dbutils.jobs.taskValues.get(taskKey = 'gold_task', key = 'layer_vars_key')\n",
    "\n",
    "gold_reports = vars['gold_reports']\n",
    "\n",
    "local_geojson_path = '/dbfs/tmp/reports.geojson'\n",
    "consumption_path = 'abfss://consumption@storagegemeente.dfs.core.windows.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34300a33-98a7-42be-b7e9-285b3aa2def4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read gold table into spark df\n",
    "gold_df = spark.read.table(gold_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf4fff5-ac18-4d96-a62f-91cfa32d281d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# all cases that are solved\n",
    "gold_df_solved = gold_df.filter(F.col('status') == 'solved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393fccaf-cc75-4484-8d3b-b98120901421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for the consumption layer we only want cases that are still open\n",
    "gold_df_filtered = gold_df.filter(F.col('status') == 'open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c3487c-8655-4dce-8c1f-361c78d542b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# further filter to only include required columns for consumption\n",
    "# create fitting date format\n",
    "gold_df_filtered = gold_df_filtered.select(\n",
    "    'id',\n",
    "    'problem_norm',\n",
    "    'prioriteit',\n",
    "    'street_name',\n",
    "    'house_number',\n",
    "    'postcode',\n",
    "    'lat',\n",
    "    'lon',\n",
    "    F.date_format('reported_on', 'dd-MM-yyyy HH:mm').alias('reported_on')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fbd5c01-3016-4c7e-a84e-5b731b3f650b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5860913143568507>, line 33\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# # convert to pandas \u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# pdf = gold_df_filtered.toPandas()\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# # save geojson dataset to local tmp storage first, hen we copy it to adls\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# # we overwrite the previous file to remove the solved cases from the map\u001B[39;00m\n",
       "\u001B[0;32m---> 33\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mput(local_geojson_path, json\u001B[38;5;241m.\u001B[39mdumps(geojson_df), \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m     35\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mcp(local_geojson_path, consumption_path, \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/json/__init__.py:231\u001B[0m, in \u001B[0;36mdumps\u001B[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;66;03m# cached encoder\u001B[39;00m\n",
       "\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m skipkeys \u001B[38;5;129;01mand\u001B[39;00m ensure_ascii \u001B[38;5;129;01mand\u001B[39;00m\n",
       "\u001B[1;32m    228\u001B[0m     check_circular \u001B[38;5;129;01mand\u001B[39;00m allow_nan \u001B[38;5;129;01mand\u001B[39;00m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m separators \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n",
       "\u001B[1;32m    230\u001B[0m     default \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sort_keys \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n",
       "\u001B[0;32m--> 231\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_encoder\u001B[38;5;241m.\u001B[39mencode(obj)\n",
       "\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONEncoder\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/json/encoder.py:200\u001B[0m, in \u001B[0;36mJSONEncoder.encode\u001B[0;34m(self, o)\u001B[0m\n",
       "\u001B[1;32m    196\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m encode_basestring(o)\n",
       "\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001B[39;00m\n",
       "\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001B[39;00m\n",
       "\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001B[39;00m\n",
       "\u001B[0;32m--> 200\u001B[0m chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterencode(o, _one_shot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chunks, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
       "\u001B[1;32m    202\u001B[0m     chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(chunks)\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/json/encoder.py:258\u001B[0m, in \u001B[0;36mJSONEncoder.iterencode\u001B[0;34m(self, o, _one_shot)\u001B[0m\n",
       "\u001B[1;32m    253\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    254\u001B[0m     _iterencode \u001B[38;5;241m=\u001B[39m _make_iterencode(\n",
       "\u001B[1;32m    255\u001B[0m         markers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault, _encoder, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindent, floatstr,\n",
       "\u001B[1;32m    256\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey_separator, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitem_separator, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msort_keys,\n",
       "\u001B[1;32m    257\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mskipkeys, _one_shot)\n",
       "\u001B[0;32m--> 258\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _iterencode(o, \u001B[38;5;241m0\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/json/encoder.py:180\u001B[0m, in \u001B[0;36mJSONEncoder.default\u001B[0;34m(self, o)\u001B[0m\n",
       "\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault\u001B[39m(\u001B[38;5;28mself\u001B[39m, o):\n",
       "\u001B[1;32m    162\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001B[39;00m\n",
       "\u001B[1;32m    163\u001B[0m \u001B[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001B[39;00m\n",
       "\u001B[1;32m    164\u001B[0m \u001B[38;5;124;03m    (to raise a ``TypeError``).\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    178\u001B[0m \n",
       "\u001B[1;32m    179\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 180\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mObject of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mo\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\u001B[1;32m    181\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis not JSON serializable\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: Object of type DataFrame is not JSON serializable"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "Object of type DataFrame is not JSON serializable"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5860913143568507>, line 33\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# # convert to pandas \u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# pdf = gold_df_filtered.toPandas()\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# # save geojson dataset to local tmp storage first, hen we copy it to adls\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# # we overwrite the previous file to remove the solved cases from the map\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mput(local_geojson_path, json\u001B[38;5;241m.\u001B[39mdumps(geojson_df), \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     35\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mcp(local_geojson_path, consumption_path, \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
        "File \u001B[0;32m/usr/lib/python3.12/json/__init__.py:231\u001B[0m, in \u001B[0;36mdumps\u001B[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;66;03m# cached encoder\u001B[39;00m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m skipkeys \u001B[38;5;129;01mand\u001B[39;00m ensure_ascii \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    228\u001B[0m     check_circular \u001B[38;5;129;01mand\u001B[39;00m allow_nan \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m separators \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    230\u001B[0m     default \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sort_keys \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[0;32m--> 231\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_encoder\u001B[38;5;241m.\u001B[39mencode(obj)\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONEncoder\n",
        "File \u001B[0;32m/usr/lib/python3.12/json/encoder.py:200\u001B[0m, in \u001B[0;36mJSONEncoder.encode\u001B[0;34m(self, o)\u001B[0m\n\u001B[1;32m    196\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m encode_basestring(o)\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterencode(o, _one_shot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chunks, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m    202\u001B[0m     chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(chunks)\n",
        "File \u001B[0;32m/usr/lib/python3.12/json/encoder.py:258\u001B[0m, in \u001B[0;36mJSONEncoder.iterencode\u001B[0;34m(self, o, _one_shot)\u001B[0m\n\u001B[1;32m    253\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    254\u001B[0m     _iterencode \u001B[38;5;241m=\u001B[39m _make_iterencode(\n\u001B[1;32m    255\u001B[0m         markers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault, _encoder, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindent, floatstr,\n\u001B[1;32m    256\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey_separator, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitem_separator, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msort_keys,\n\u001B[1;32m    257\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mskipkeys, _one_shot)\n\u001B[0;32m--> 258\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _iterencode(o, \u001B[38;5;241m0\u001B[39m)\n",
        "File \u001B[0;32m/usr/lib/python3.12/json/encoder.py:180\u001B[0m, in \u001B[0;36mJSONEncoder.default\u001B[0;34m(self, o)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault\u001B[39m(\u001B[38;5;28mself\u001B[39m, o):\n\u001B[1;32m    162\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001B[39;00m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001B[39;00m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;124;03m    (to raise a ``TypeError``).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    178\u001B[0m \n\u001B[1;32m    179\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 180\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mObject of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mo\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    181\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis not JSON serializable\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
        "\u001B[0;31mTypeError\u001B[0m: Object of type DataFrame is not JSON serializable"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert to pandas \n",
    "pdf = gold_df_filtered.toPandas()\n",
    "\n",
    "# convert to geojson structure\n",
    "# iterrows wil return a tuple with index and row\n",
    "features = []\n",
    "for index, row in pdf.iterrows():\n",
    "    features.append({\n",
    "        'type': 'Feature',\n",
    "        'geometry': {\n",
    "            'type': 'Point',\n",
    "            'coordinates': [row['lon'], row['lat']]\n",
    "        },\n",
    "        'properties': {\n",
    "            'id': row['id'],\n",
    "            'problem': row['problem_norm'],\n",
    "            'street_name': row['street_name'],\n",
    "            'house_number': row['house_number'],\n",
    "            'postcode': row['postcode'],\n",
    "            'priority': row['prioriteit'],\n",
    "            'reported_on': str(row['reported_on'])\n",
    "        }\n",
    "    })\n",
    "\n",
    "geojson = {\n",
    "    'type': 'FeatureCollection',\n",
    "    'features': features\n",
    "}\n",
    "\n",
    "# spark can not directly write geojson to adls\n",
    "# save geojson dataset to local tmp storage first, hen we copy it to adls\n",
    "# we overwrite the previous file to remove the solved cases from the map\n",
    "dbutils.fs.put(local_geojson_path, json.dumps(geojson), True)\n",
    "\n",
    "dbutils.fs.cp(local_geojson_path, consumption_path, True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Consumption Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}